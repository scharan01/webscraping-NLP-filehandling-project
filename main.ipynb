{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "import spacy\n",
    "from textstat.textstat import textstatistics\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the data from input excel file into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"input.xlsx\")\n",
    "df = pd.DataFrame(data)\n",
    "data = {\"URL_ID\" : [],\"URL\" : [],\"articles\" : []}\n",
    "datalist = pd.DataFrame(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping the article text from the links and creating text files for each URL_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error in loading the page as its not available\n",
      "error in loading the page as its not available\n",
      "error in loading the page as its not available\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for ind,row in df.iterrows() :\n",
    "    \n",
    "    try : \n",
    "        url = row['URL']\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "        result = requests.get(url = url,headers = headers).text\n",
    "        \n",
    "        doc = BeautifulSoup(result, \"html.parser\")\n",
    "        article = doc.find(class_ = \"td-post-content\")\n",
    "        article_final = \"\"\n",
    "\n",
    "        for ar in article.find_all(\"p\") : \n",
    "            article_final += ar.text\n",
    "\n",
    "        file = str(int(row[\"URL_ID\"])) + \".txt\"\n",
    "\n",
    "        dd = {\"URL_ID\" : [int(row[\"URL_ID\"])],\"URL\" : [row[\"URL\"]],\"articles\" : [article_final]}\n",
    "        dd1 = pd.DataFrame(dd)\n",
    "        #datalist = datalist.append(dd,ignore_index=True)\n",
    "        datalist = pd.concat([datalist,dd1],ignore_index=True)\n",
    "        \n",
    "        f = open(file,\"w+\")\n",
    "        f.write(article_final)\n",
    "        f.close()\n",
    "\n",
    "    except :\n",
    "        print(\"error in loading the page as its not available\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding all the stopwords in a list using the given files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stpwrds = []\n",
    "f = open('C:/Users/satya/Desktop/blackcoffrintern/StopWords-20221230T110845Z-001/StopWords/StopWords_Auditor.txt', 'r')\n",
    "adtr = f.read().splitlines()\n",
    "f.close()\n",
    "adtr_l = [name.lower() for name in adtr]\n",
    "stpwrds.extend(adtr_l)\n",
    "\n",
    "f = open('C:/Users/satya/Desktop/blackcoffrintern/StopWords-20221230T110845Z-001\\StopWords\\StopWords_Currencies.txt', 'r')\n",
    "curr = f.read().splitlines()\n",
    "f.close()\n",
    "curr_s = [cur.split(\"|\") for cur in curr]\n",
    "curr_m = []\n",
    "for i in curr_s :\n",
    "    curr_m.append(i[0].strip().lower())\n",
    "stpwrds.extend(curr_m)\n",
    "\n",
    "f = open('C:/Users/satya/Desktop/blackcoffrintern/StopWords-20221230T110845Z-001\\StopWords\\StopWords_DatesandNumbers.txt', 'r')\n",
    "dnm = f.read().splitlines()\n",
    "f.close()\n",
    "dnm_s = [dn.split(\"|\") for dn in dnm]\n",
    "dnm_m = []\n",
    "for i in dnm_s :\n",
    "    dnm_m.append(i[0].strip().lower())\n",
    "stpwrds.extend(dnm_m)\n",
    "\n",
    "f = open('C:/Users/satya/Desktop/blackcoffrintern/StopWords-20221230T110845Z-001\\StopWords\\StopWords_Generic.txt', 'r')\n",
    "gen = f.read().splitlines()\n",
    "f.close()\n",
    "gen_l = [name.lower() for name in gen]\n",
    "stpwrds.extend(gen_l)\n",
    "\n",
    "f = open('C:/Users/satya/Desktop/blackcoffrintern/StopWords-20221230T110845Z-001\\StopWords\\StopWords_GenericLong.txt', 'r')\n",
    "gens = f.read().splitlines()\n",
    "f.close()\n",
    "gens_l = [name.lower() for name in gens]\n",
    "stpwrds.extend(gens_l)\n",
    "\n",
    "f = open('C:/Users/satya/Desktop/blackcoffrintern/StopWords-20221230T110845Z-001\\StopWords\\StopWords_Geographic.txt', 'r')\n",
    "gps = f.read().splitlines()\n",
    "f.close()\n",
    "gps_s = [dn.split(\"|\") for dn in gps]\n",
    "gps_m = []\n",
    "for i in gps_s :\n",
    "    gps_m.append(i[0].strip().lower())\n",
    "stpwrds.extend(gps_m)\n",
    "\n",
    "f = open('C:/Users/satya/Desktop/blackcoffrintern/StopWords-20221230T110845Z-001\\StopWords\\StopWords_Geographic.txt', 'r')\n",
    "nms = f.read().splitlines()\n",
    "f.close()\n",
    "nms_s = [dn.split(\"|\") for dn in nms]\n",
    "nms_m = []\n",
    "for i in nms_s :\n",
    "    nms_m.append(i[0].strip().lower())\n",
    "stpwrds.extend(nms_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37.0</td>\n",
       "      <td>https://insights.blackcoffer.com/ai-in-healthc...</td>\n",
       "      <td>introduction“if anything kills over 10 million...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38.0</td>\n",
       "      <td>https://insights.blackcoffer.com/what-if-the-c...</td>\n",
       "      <td>human minds, a fascination in itself carrying ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.0</td>\n",
       "      <td>https://insights.blackcoffer.com/what-jobs-wil...</td>\n",
       "      <td>introductionai is rapidly evolving in the empl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.0</td>\n",
       "      <td>https://insights.blackcoffer.com/will-machine-...</td>\n",
       "      <td>“anything that could give rise to smarter-than...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41.0</td>\n",
       "      <td>https://insights.blackcoffer.com/will-ai-repla...</td>\n",
       "      <td>“machine intelligence is the last invention th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>146.0</td>\n",
       "      <td>https://insights.blackcoffer.com/blockchain-fo...</td>\n",
       "      <td>reconciling with the financial realities of an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>147.0</td>\n",
       "      <td>https://insights.blackcoffer.com/the-future-of...</td>\n",
       "      <td>an investment is a resource or thing procured ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>148.0</td>\n",
       "      <td>https://insights.blackcoffer.com/big-data-anal...</td>\n",
       "      <td>quality and affordable healthcare is a vision ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>149.0</td>\n",
       "      <td>https://insights.blackcoffer.com/business-anal...</td>\n",
       "      <td>analytics is a statistical scientific process ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>150.0</td>\n",
       "      <td>https://insights.blackcoffer.com/challenges-an...</td>\n",
       "      <td>to begin with i shall first like to explain wh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>111 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     URL_ID                                                URL  \\\n",
       "0      37.0  https://insights.blackcoffer.com/ai-in-healthc...   \n",
       "1      38.0  https://insights.blackcoffer.com/what-if-the-c...   \n",
       "2      39.0  https://insights.blackcoffer.com/what-jobs-wil...   \n",
       "3      40.0  https://insights.blackcoffer.com/will-machine-...   \n",
       "4      41.0  https://insights.blackcoffer.com/will-ai-repla...   \n",
       "..      ...                                                ...   \n",
       "106   146.0  https://insights.blackcoffer.com/blockchain-fo...   \n",
       "107   147.0  https://insights.blackcoffer.com/the-future-of...   \n",
       "108   148.0  https://insights.blackcoffer.com/big-data-anal...   \n",
       "109   149.0  https://insights.blackcoffer.com/business-anal...   \n",
       "110   150.0  https://insights.blackcoffer.com/challenges-an...   \n",
       "\n",
       "                                              articles  \n",
       "0    introduction“if anything kills over 10 million...  \n",
       "1    human minds, a fascination in itself carrying ...  \n",
       "2    introductionai is rapidly evolving in the empl...  \n",
       "3    “anything that could give rise to smarter-than...  \n",
       "4    “machine intelligence is the last invention th...  \n",
       "..                                                 ...  \n",
       "106  reconciling with the financial realities of an...  \n",
       "107  an investment is a resource or thing procured ...  \n",
       "108  quality and affordable healthcare is a vision ...  \n",
       "109  analytics is a statistical scientific process ...  \n",
       "110  to begin with i shall first like to explain wh...  \n",
       "\n",
       "[111 rows x 3 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = datalist.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making all the article text lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['articles'] = df1['articles'].str.lower()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing punctuations and stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "      return re.sub(r'[^\\w\\s]',' ',text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "      text = ' '.join([word for word in text.split() if word not in (stpwrds)])\n",
    "      return text\n",
    "\n",
    "df1['articles'] = df1['articles'].apply(lambda x : remove_punctuation(x))\n",
    "df1['articles'] = df1['articles'].apply(lambda x : remove_stopwords(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the positive and negative dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('C:/Users/satya/Desktop/blackcoffrintern/MasterDictionary-20221230T110842Z-001/MasterDictionary/positive-words.txt', 'r')\n",
    "pos = f.read().splitlines()\n",
    "f.close()\n",
    "\n",
    "n = open('C:/Users/satya/Desktop/blackcoffrintern/MasterDictionary-20221230T110842Z-001/MasterDictionary/negative-words.txt', 'r')\n",
    "neg = n.read().splitlines()\n",
    "n.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function for computing indeces requiring cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_indeces(text) :\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    pol_score = 0\n",
    "    sub_score = 0\n",
    "    wor_count = 0\n",
    "\n",
    "    for token in tokens :\n",
    "        if token in pos :\n",
    "            pos_score += 1\n",
    "        elif token in neg :\n",
    "            neg_score += 1\n",
    "\n",
    "    pol_score = (pos_score - neg_score)/((pos_score+neg_score)+0.000001)\n",
    "    sub_score = (pos_score+neg_score)/((len(tokens)+0.000001))\n",
    "\n",
    "    result = {\"positive score\" : pos_score,\"negative score\" : neg_score,\"polarity score\" : pol_score,\"subjectivity score\" : sub_score,\"word count\" : len(tokens)}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "functions for computing other indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_count(text):\n",
    "    sentences = break_sentences(text)\n",
    "    words = 0\n",
    "    for sentence in sentences:\n",
    "        words += len([token for token in sentence])\n",
    "    return words\n",
    "\n",
    "def break_sentences(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    return list(doc.sents)\n",
    "\n",
    "def syllables_count(word):\n",
    "    return textstatistics().syllable_count(word)\n",
    "\n",
    "def avg_sentence_length(text):\n",
    "    words = word_count(text)\n",
    "    sentences = sentence_count(text)\n",
    "    average_sentence_length = float(words / sentences)\n",
    "    return average_sentence_length\n",
    "\n",
    "def sentence_count(text):\n",
    "    sentences = break_sentences(text)\n",
    "    return len(sentences)\n",
    "\n",
    "def complex_words(text):\n",
    "     \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    words = []\n",
    "    sentences = break_sentences(text)\n",
    "    for sentence in sentences:\n",
    "        words += [str(token) for token in sentence]\n",
    " \n",
    "    diff_words_set = set()\n",
    "     \n",
    "    for word in words:\n",
    "        syllable_count = syllables_count(word)\n",
    "        if word not in nlp.Defaults.stop_words and syllable_count >= 2:\n",
    "            diff_words_set.add(word)\n",
    " \n",
    "    return len(diff_words_set)\n",
    "\n",
    "\n",
    "def gunning_fog(text):\n",
    "    per_cmplx_words = (complex_words(text) / word_count(text) * 100)\n",
    "    grade = 0.4 * (avg_sentence_length(text) + per_cmplx_words)\n",
    "    return grade\n",
    "\n",
    "def syllable_count_word(text) :\n",
    "    \n",
    "    words = []\n",
    "    sentences = break_sentences(text)\n",
    "    syllable_count = 0\n",
    "    for sentence in sentences:\n",
    "        words += [str(token) for token in sentence]\n",
    "\n",
    "    for word in words:\n",
    "        syllable_count += syllables_count(word)\n",
    "    \n",
    "    return syllable_count\n",
    "\n",
    "def avg_word_length(text) :\n",
    "    ans = len(text)/word_count(text)\n",
    "    return ans\n",
    "\n",
    "def personal_pronouns(text) :\n",
    "    pronounRegex = re.compile(r'\\b(I|we|my|ours|(?-i:us))\\b',re.I)\n",
    "    pronouns = pronounRegex.findall(text)\n",
    "    return len(pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_score = []\n",
    "neg_score = []\n",
    "pol_score = []\n",
    "sub_score = []\n",
    "avg_senlen = []\n",
    "per_cmplx = []\n",
    "fog_index = []\n",
    "avg_wrdspsen = []\n",
    "cmplx_wordcnt = []\n",
    "wrd_cnt = []\n",
    "syll_pwrd = []\n",
    "per_pro = []\n",
    "avg_wrdlen = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind,row in df1.iterrows() :\n",
    "\n",
    "    ans = compute_indeces(row[\"articles\"])\n",
    "    pos_score.append(ans[\"positive score\"])\n",
    "    neg_score.append(ans[\"negative score\"])\n",
    "    pol_score.append(ans[\"polarity score\"])\n",
    "    sub_score.append(ans[\"subjectivity score\"])\n",
    "    wrd_cnt.append(ans[\"word count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = datalist.copy()\n",
    "for ind,row in df2.iterrows() :\n",
    "    asl = avg_sentence_length(row[\"articles\"])\n",
    "    pcw = (complex_words(row[\"articles\"])/word_count(row[\"articles\"]))*100\n",
    "    fi = gunning_fog(row[\"articles\"])\n",
    "    anwps = avg_sentence_length(row[\"articles\"])\n",
    "    cpw = complex_words(row[\"articles\"])\n",
    "    scw = syllable_count_word(row[\"articles\"])\n",
    "    awl = avg_word_length(row[\"articles\"])\n",
    "    pp = personal_pronouns(row[\"articles\"])\n",
    "    \n",
    "    avg_senlen.append(asl)\n",
    "    per_cmplx.append(pcw)\n",
    "    fog_index.append(fi)\n",
    "    avg_wrdspsen.append(anwps)\n",
    "    cmplx_wordcnt.append(cpw)\n",
    "    syll_pwrd.append(scw)\n",
    "    per_pro.append(pp)\n",
    "    avg_wrdlen.append(awl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding all the indeces in the final dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.drop(['articles'],axis=1,inplace=True)\n",
    "df2['POSITIVE SCORE'] = pos_score\n",
    "df2['NEGATIVE SCORE'] = neg_score\n",
    "df2['POLARITY SCORE'] = pol_score\n",
    "df2['SUBJECTIVITY SCORE'] = sub_score\n",
    "df2['AVG SENTENCE LENGTH'] = avg_senlen\n",
    "df2['PERCENTAGE OF COMPLEX WORDS'] = per_cmplx\n",
    "df2['FOG INDEX'] = fog_index\n",
    "df2['AVERAGE NO OF WORDS PER SENTENCE'] = avg_wrdspsen\n",
    "df2['COMPLEX WORD COUNT'] = cmplx_wordcnt\n",
    "df2['WORD COUNT'] = wrd_cnt\n",
    "df2['SYLLABLE PER WORD'] = syll_pwrd\n",
    "df2['PERSONAL PRONOUNS'] = per_pro\n",
    "df2['AVERAGE WORD LENGTH'] = avg_wrdlen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the dataframe into a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"output.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3f68dd0345951f0cf8ba50f1a3a6916c07782ad147069f612168b176ca281376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
